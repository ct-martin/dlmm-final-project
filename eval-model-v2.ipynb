{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This code will output a Markdown file with an evaluation report for each model in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  {\"name\": \"blip-image-captioning-base-8bit-pre-peft\", \"int8\": True, \"peft\": False, \"noToDevice\": True}, # Salesforce BLIP model in int8, no PEFT\n",
    "  {\"name\": \"blip-image-captioning-base-quantized\", \"int8\": True, \"peft\": True, \"noToDevice\": False},      # Salesforce BLIP int8+PEFT\n",
    "  {\"name\": \"blip-image-captioning-base-finetuned\", \"int8\": True, \"peft\": True, \"noToDevice\": False},      # Model from training\n",
    "  {\"name\": \"Salesforce/blip-image-captioning-base\", \"int8\": False, \"peft\": False, \"noToDevice\": False},   # Base model (full not-int8)\n",
    "]\n",
    "\n",
    "test_filename = \"ids_test.csv\"\n",
    "\n",
    "report_sample_size = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep & Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Image\n",
    "\n",
    "def prep_data(df):\n",
    "  files = [f\"media/{media_id}.jpg\" for media_id in df['media_id'].to_list()]\n",
    "  descriptions = [text for text in df['description'].to_list()]\n",
    "\n",
    "  return Dataset.from_dict({ \"image\": files, \"text\": descriptions }).cast_column(\"image\", Image())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DataFrame\n",
    "df_test = pd.read_csv(test_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_dataset(processor):\n",
    "#  print(\"Loading Dataset & setting processor\")\n",
    "#\n",
    "#  # Create Dataset from DataFrame\n",
    "#  ds_test = prep_data(df_test)\n",
    "#\n",
    "#  # Define transform function using processor\n",
    "#  # https://huggingface.co/docs/transformers/tasks/image_captioning\n",
    "#  def transforms(example_batch):\n",
    "#      images = [x.convert(\"RGB\").resize((100,100)) for x in example_batch[\"image\"]]\n",
    "#      captions = [x for x in example_batch[\"text\"]]\n",
    "#      inputs = processor(images=images, text=captions, padding=\"max_length\", return_tensors=\"pt\")\n",
    "#      #inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "#      #inputs = processor(images, return_tensors=\"pt\").to(device)\n",
    "#      return inputs\n",
    "#\n",
    "#  # Set transform\n",
    "#  ds_test.set_transform(transforms)\n",
    "#\n",
    "#  # Return Dataset with transform\n",
    "#  return ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = prep_data(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('report.md','w', encoding=\"utf-8\")\n",
    "\n",
    "def out(text):\n",
    "  global file\n",
    "  #print(text)\n",
    "  file.write(text+\"\\n\")\n",
    "\n",
    "out(\"# Model Evaluation Report\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out(\"## Dataset Sample\")\n",
    "out(\"| Image | Description |\")\n",
    "out(\"|-------|-------------|\")\n",
    "\n",
    "sample_images = df_test['media_id'].head(report_sample_size).to_list()\n",
    "sample_descriptions = df_test['description'].head(5).to_list()\n",
    "\n",
    "for i in range(len(sample_images)):\n",
    "  out(f\"| ![](media/{sample_images[i]}.jpg) | {sample_descriptions[i]} |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from peft import LoraConfig, LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
    "\n",
    "#PEFT Config, this should match training\n",
    "peft_config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"qkv\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "def load_model(model_data):\n",
    "  model_name = model_data['name']\n",
    "  int8 = model_data['int8']\n",
    "  peft = model_data['peft']\n",
    "\n",
    "  print(f\"Loading model {model_name}\")\n",
    "\n",
    "  processor = BlipProcessor.from_pretrained(model_name)\n",
    "  model = BlipForConditionalGeneration.from_pretrained(model_name, load_in_8bit=int8)\n",
    "\n",
    "  #if int8==True:\n",
    "  #  model = prepare_model_for_int8_training(model)\n",
    "  \n",
    "  if peft==True:\n",
    "    peft_config = PeftConfig.from_pretrained(model_name)\n",
    "    model = PeftModel.from_pretrained(model, model_name, config=peft_config)\n",
    "    #model = get_peft_model(model, peft_config)\n",
    "    print(model.device)\n",
    "  \n",
    "  if model_data['noToDevice'] == False:\n",
    "    model = model.to(device)\n",
    "\n",
    "  return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 01:49:53.482358: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-29 01:49:53.504439: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-29 01:49:53.504457: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-29 01:49:53.505034: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-29 01:49:53.508576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 01:49:53.978462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/christian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "wer = evaluate.load(\"wer\")\n",
    "meteor = evaluate.load(\"meteor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(model, processor, data):\n",
    "  image = data['image'].convert('RGB').resize((100,100))\n",
    "  inputs = processor(image, return_tensors=\"pt\")\n",
    "  pixels = inputs['pixel_values']\n",
    "  #print(\"Model device:\",model.device)\n",
    "  #print(\"Pixel Tensor Device:\",pixels.get_device())\n",
    "  #if pixels.get_device() != model.device:\n",
    "    #print(\"Attempting to move Tensor to model's device\")\n",
    "  pixels = pixels.to(model.device)\n",
    "  #print(\"Pixel Tensor Device:\",pixels.get_device())\n",
    "  generated_tensor = model.generate(pixel_values=pixels, max_length=200)\n",
    "  decoded_desc = processor.batch_decode(generated_tensor, skip_special_tokens=True)[0]\n",
    "  return decoded_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_descriptions(descriptions_human, descriptions_generated):\n",
    "  metrics = {}\n",
    "  rouge_score = rouge.compute(predictions=descriptions_generated, references=descriptions_human)\n",
    "  for k, v in rouge_score.items():\n",
    "    metrics[k] = v\n",
    "  bleu_score = bleu.compute(predictions=descriptions_generated, references=descriptions_human)\n",
    "  for k, v in bleu_score.items():\n",
    "    metrics['bleu_'+k] = v\n",
    "  wer_score = wer.compute(predictions=descriptions_generated, references=descriptions_human)\n",
    "  metrics['wer'] = wer_score\n",
    "  meteor_score = meteor.compute(predictions=descriptions_generated, references=descriptions_human)\n",
    "  for k, v in meteor_score.items():\n",
    "    metrics[k] = v\n",
    "  \n",
    "  #print(metrics)\n",
    "\n",
    "  return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_data):\n",
    "  model_name = model_data['name']\n",
    "  out(f\"## Model: {model_name}\")\n",
    "  \n",
    "  model, processor = load_model(model_data)\n",
    "\n",
    "  print(\"Model device:\",model.device)\n",
    "\n",
    "  print('Generating Descriptions')\n",
    "  descriptions_human = []\n",
    "  descriptions_generated = []\n",
    "\n",
    "  for i in ds_test:\n",
    "    descriptions_human.append(i['text'])\n",
    "    generated_text = generate_description(model, processor, i)\n",
    "    descriptions_generated.append(generated_text)\n",
    "  \n",
    "  out(\"Sample Descriptions:\")\n",
    "  out(\"| Image | Generated Description | Human Description |\")\n",
    "  out(\"|-|-|-|\")\n",
    "  for i in range(report_sample_size):\n",
    "    out(f\"| ![](media/{sample_images[i]}.jpg) | {descriptions_generated[i]} | {descriptions_human[i]} |\")\n",
    "  \n",
    "  # Newline\n",
    "  out(\"\")\n",
    "\n",
    "  print(\"Evaluating\")\n",
    "  metrics = evaluate_descriptions(descriptions_human, descriptions_generated)\n",
    "\n",
    "  out(\"Metrics:\")\n",
    "  out(\"| Metric | Score |\")\n",
    "  out(\"|--------|-------|\")\n",
    "  for metric_name, metric_score in metrics.items():\n",
    "    out(f\"| {metric_name} | `{metric_score}` |\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model blip-image-captioning-base-8bit-pre-peft\n",
      "Model device: cuda:0\n",
      "Generating Descriptions\n",
      "Evaluating\n",
      "Loading model blip-image-captioning-base-quantized\n",
      "cuda:0\n",
      "Model device: cuda:0\n",
      "Generating Descriptions\n",
      "Evaluating\n",
      "Loading model blip-image-captioning-base-finetuned\n",
      "cuda:0\n",
      "Model device: cuda:0\n",
      "Generating Descriptions\n",
      "Evaluating\n",
      "Loading model Salesforce/blip-image-captioning-base\n",
      "Model device: cuda:0\n",
      "Generating Descriptions\n",
      "Evaluating\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "#for model_metadata in [models[0]]:\n",
    "for model_metadata in models:\n",
    "  evaluate_model(model_metadata)\n",
    "  gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
